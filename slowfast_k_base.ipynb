{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "slowfast_k_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1eJ2QY86m6DS3d1XufMNhyDT4Xw9NcK9a",
      "authorship_tag": "ABX9TyMSMfWmn9SBLZ7Ujx3idFtT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfqbasha/CSCE636_DL2/blob/master/slowfast_k_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARe2V1Fd5nBR",
        "colab_type": "code",
        "outputId": "73a6eab9-6ff2-4506-b3f1-1f2d19d6f21f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Go to the Directory\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/CSCE636/slowfast-keras2Class/slowfast-keras"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/CSCE636/slowfast-keras2Class/slowfast-keras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_zVJ2AwNxrs",
        "colab_type": "code",
        "outputId": "0b68aa88-6a5b-4f5f-cf24-fe3136bb29b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#Pre-processing script\n",
        "!python utils/ucf_hmdb51_frames.py ../../Data/data_clips_100_ref_test_jpg/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "../../Data/data_clips_100_ref_test_jpg/opening_refrigerator/D0XyROrTMKU_000010_000020 117\n",
            "../../Data/data_clips_100_ref_test_jpg/opening_refrigerator/dXGdWJITkdU_000014_000024 250\n",
            "../../Data/data_clips_100_ref_test_jpg/opening_refrigerator/eexhb-2WwYo_000018_000028 250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRvGQobVu3SK",
        "colab_type": "text"
      },
      "source": [
        "**TRAIN_CELL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOhaC7hCITbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import multi_gpu_model\n",
        "from model import nets\n",
        "from model import slowfast\n",
        "from opts import parse_opts\n",
        "from utils import get_optimizer, SGDRScheduler_with_WarmUp, TrainPrint, PrintLearningRate, ParallelModelCheckpoint\n",
        "from dataset.dataset import DataGenerator\n",
        "\n",
        "class Args:\n",
        "  root_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/slowfast-keras2Class/slowfast-keras'\n",
        "  video_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref_jpg'\n",
        "  name_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref/classInd.txt'\n",
        "  train_list = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref/train.txt'\n",
        "  val_list = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref/test.txt'\n",
        "  result_path = 'results'\n",
        "  data_name = 'ntu'\n",
        "  gpus = [0]\n",
        "  log_dir = 'log'\n",
        "  num_classes = 2\n",
        "  crop_size = 224\n",
        "  clip_len = 64\n",
        "  short_side = [256, 320]\n",
        "  n_samples_for_each_video = 1\n",
        "  lr = 0.00001\n",
        "  momentum = 0.9\n",
        "  weight_decay = 1e-4\n",
        "  lr_decay = 0.8\n",
        "  cycle_length = 10\n",
        "  multi_factor = 1.5\n",
        "  warm_up_epoch = 5\n",
        "  optimizer = 'SGD'\n",
        "  batch_size = 4\n",
        "  epochs = 2\n",
        "  workers = 4\n",
        "  network = 'resnet50'\n",
        "  pretrained_weights = None\n",
        "  test_videos_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref_test_jpg'\n",
        "  test_list_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref/test_final.txt'\n",
        "\n",
        "\n",
        "def create_callbacks(opt, steps_per_epoch, model=None):\n",
        "    log_dir = os.path.join(opt.root_path, opt.log_dir)\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.mkdir(log_dir)\n",
        "    tensorboard = TensorBoard(log_dir=log_dir, write_graph=True)\n",
        "\n",
        "    result_path = os.path.join(opt.root_path, opt.result_path)\n",
        "    if not os.path.exists(result_path):\n",
        "        os.mkdir(result_path)\n",
        "\n",
        "    if model is not None:\n",
        "        print(\"mbashat: using ParallelModel\")\n",
        "        checkpoint = ParallelModelCheckpoint(model, os.path.join(result_path, '{epoch:03d}.h5'),\n",
        "                                    monitor='val_acc', save_weights_only=True, save_best_only=True, period=1)\n",
        "    else:\n",
        "        print(\"mbashat: using ModelCheckpoint\")\n",
        "        checkpoint = ModelCheckpoint(os.path.join(result_path, '{epoch:03d}.h5'),\n",
        "                                    monitor='val_acc', save_weights_only=True, save_best_only=True, period=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_acc', min_delta=0, patience=10)\n",
        "    learning_rate_scheduler = SGDRScheduler_with_WarmUp(0, opt.lr, steps_per_epoch, lr_decay=opt.lr_decay, \n",
        "                                                        cycle_length=opt.cycle_length, multi_factor=opt.multi_factor,\n",
        "                                                        warm_up_epoch=opt.warm_up_epoch)\n",
        "    #training_print = TrainPrint(steps_per_epoch, opt.epochs)\n",
        "    print_lr = PrintLearningRate()\n",
        "\n",
        "    return [tensorboard, learning_rate_scheduler, print_lr, checkpoint, early_stopping]\n",
        "    \n",
        "\n",
        "\n",
        "def train(opt):\n",
        "    K.clear_session()\n",
        "    video_input = Input(shape=(None, None, None, 3))\n",
        "    model = nets.network[opt.network](video_input, num_classes=opt.num_classes)\n",
        "    print(\"Create {} model with {} classes\".format(opt.network, opt.num_classes))\n",
        "\n",
        "    if opt.pretrained_weights is not None:\n",
        "        model.load_weights(opt.pretrained_weights)\n",
        "        print(\"Loading weights from {}\".format(opt.pretrained_weights))\n",
        "\n",
        "    optimizer = get_optimizer(opt)\n",
        "\n",
        "    train_data_generator = DataGenerator(opt.data_name, opt.video_path, opt.train_list, opt.name_path, \n",
        "                                        'train', opt.batch_size, opt.num_classes, True, opt.short_side, \n",
        "                                        opt.crop_size, opt.clip_len, opt.n_samples_for_each_video)                                     \n",
        "    val_data_generator = DataGenerator(opt.data_name, opt.video_path, opt.val_list, opt.name_path, 'val', \n",
        "                                        opt.batch_size, opt.num_classes, False, opt.short_side, \n",
        "                                        opt.crop_size, opt.clip_len, opt.n_samples_for_each_video)\n",
        "    predict_data_generator = DataGenerator(opt.data_name, opt.test_videos_path, opt.test_list_path, opt.name_path, \n",
        "                                        'val', 1, opt.num_classes, False, opt.short_side, \n",
        "                                        opt.crop_size, opt.clip_len, opt.n_samples_for_each_video, to_fit=False)  \n",
        "    \n",
        "    \n",
        "    callbacks = create_callbacks(opt, max(1, train_data_generator.__len__()), model)\n",
        "\n",
        "    if len(opt.gpus) > 1:\n",
        "        print('Using multi gpus')\n",
        "        parallel_model = multi_gpu_model(model, gpus=len(opt.gpus))\n",
        "        parallel_model.compile(optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy'])\n",
        "        parallel_model.fit_generator(train_data_generator, steps_per_epoch=max(1, train_data_generator.__len__()),\n",
        "                            epochs=opt.epochs, validation_data=val_data_generator, validation_steps=max(1, val_data_generator.__len__()),\n",
        "                            workers=opt.workers, callbacks=callbacks)\n",
        "    else:\n",
        "        print(\"GPU <1 run\")\n",
        "        model.compile(optimizer=optimizer, loss=categorical_crossentropy, metrics=['accuracy'])\n",
        "        print(\"compile done\")\n",
        "        print(\"mbashat : Val_steps, steps_per_epoch, Val_dat\", max(1, val_data_generator.__len__()), max(1, train_data_generator.__len__()), val_data_generator)\n",
        "        global history_mbt\n",
        "        #history_mbt = model.fit_generator(train_data_generator, steps_per_epoch=max(1, train_data_generator.__len__()),\n",
        "                            #epochs=opt.epochs, validation_data=val_data_generator, validation_steps=max(1, val_data_generator.__len__()),\n",
        "                            #workers=opt.workers, callbacks=callbacks)\n",
        "        history_mbt = model.fit_generator(train_data_generator, steps_per_epoch=max(1, train_data_generator.__len__()), epochs=opt.epochs, validation_data=val_data_generator, validation_steps=max(1, val_data_generator.__len__()),\n",
        "                        workers=opt.workers)\n",
        "        \n",
        "        model.summary()\n",
        "        from tensorflow.keras.utils import plot_model\n",
        "        plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    print(\"model fit done\")\n",
        "    print(\"mbashat: saving model\")\n",
        "    model.save('SlowFast_refrigerator_200.h5')\n",
        "    print(\"mbashat: saving weights at :\", os.path.join(os.path.join(opt.root_path, opt.result_path) ))\n",
        "    model.save_weights(os.path.join(os.path.join(opt.root_path, opt.result_path), 'trained_weights_final_200.h5'))\n",
        "    print(\"mbashat: predict on val data\")\n",
        "    model_predicted = model.predict(predict_data_generator)\n",
        "    import numpy as np\n",
        "    print(np.argmax(model_predicted, axis=1))\n",
        "    print(\"mbashat: saving weights to variable\")\n",
        "    #for layer in model.layers:\n",
        "      #weights = layer.get_weights()\n",
        "      #print(weights)\n",
        "    #for video_batch, labels_batch in train_data_generator:\n",
        "      #print('video batch shape, label shape:', video_batch.shape, labels_batch)\n",
        "    %matplotlib inline\n",
        "    import matplotlib.pyplot as plt\n",
        "    acc = history_mbt.history['acc']\n",
        "    val_acc = history_mbt.history['val_acc']\n",
        "    loss = history_mbt.history['loss']\n",
        "    val_loss = history_mbt.history['val_loss']\n",
        "    epochs = range(1, len(acc) + 1)\n",
        "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print('printking Keys in history')\n",
        "    #for key in history_mbt.history:\n",
        "      #print(key)\n",
        "    #print(\"end of code\")\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    opt = Args()\n",
        "    print(opt)\n",
        "    if len(opt.gpus) > 1:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = \",\".join(map(str, opt.gpus))\n",
        "    train(opt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_eZ4DTUwCZh",
        "colab_type": "text"
      },
      "source": [
        "**TEST_CELL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb_dX3aRe_Fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import multi_gpu_model\n",
        "from model import nets\n",
        "from model import slowfast\n",
        "from opts import parse_opts\n",
        "from utils import get_optimizer, SGDRScheduler_with_WarmUp, TrainPrint, PrintLearningRate, ParallelModelCheckpoint\n",
        "from dataset.dataset import DataGenerator\n",
        "class Args:\n",
        "  root_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/slowfast-keras2Class/slowfast-keras'\n",
        "  video_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref_jpg'\n",
        "  name_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref/classInd.txt'\n",
        "  train_list = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref/train.txt'\n",
        "  val_list = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref/test.txt'\n",
        "  result_path = 'results'\n",
        "  data_name = 'ntu'\n",
        "  gpus = [0]\n",
        "  log_dir = 'log'\n",
        "  num_classes = 2\n",
        "  crop_size = 224\n",
        "  clip_len = 64\n",
        "  short_side = [256, 320]\n",
        "  n_samples_for_each_video = 1\n",
        "  lr = 0.00001\n",
        "  momentum = 0.9\n",
        "  weight_decay = 1e-4\n",
        "  lr_decay = 0.8\n",
        "  cycle_length = 10\n",
        "  multi_factor = 1.5\n",
        "  warm_up_epoch = 5\n",
        "  optimizer = 'SGD'\n",
        "  batch_size = 4\n",
        "  epochs = 30\n",
        "  workers = 4\n",
        "  network = 'resnet50'\n",
        "  pretrained_weights = None\n",
        "  test_videos_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref_test_jpg'\n",
        "  test_list_path = '/content/drive/My Drive/Colab Notebooks/CSCE636/Data/data_clips_100_ref/test_final.txt'\n",
        "\n",
        "opt = Args()\n",
        "# load model\n",
        "model = tf.keras.models.load_model('SlowFast_refrigerator.h5')\n",
        "# summarize model.\n",
        "model.summary()\n",
        "predict_data_generator = DataGenerator(opt.data_name, opt.test_videos_path, opt.test_list_path, opt.name_path, \n",
        "                                        'val', 1, opt.num_classes, False, opt.short_side, \n",
        "                                        opt.crop_size, opt.clip_len, opt.n_samples_for_each_video, to_fit=False)  \n",
        "model_predicted = model.predict(predict_data_generator)\n",
        "import numpy as np\n",
        "print(np.argmax(model_predicted, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}